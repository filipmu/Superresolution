{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply AI Superresolution to video - Train VDSR from Video Frames\n",
    "\n",
    "## Use a perception loss\n",
    "https://github.com/richzhang/PerceptualSimilarity\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get a video dataset to do better training\n",
    "- higher resolution than used in research\n",
    "\n",
    "### super 8 look https://www.youtube.com/watch?v=7Q5UZmwxIXo\n",
    "- low focus (diffusion and blur - bokah)\n",
    "- low contrast\n",
    "- camera is more shakey on home movies\n",
    "- high depth of field everything is sharp\n",
    "- 4x3\n",
    "- film grain\n",
    "- color changes\n",
    "- mpeg2 compression noise\n",
    "\n",
    "### VCR look\n",
    "- jitter on scan lines - see http://www.avisynth.nl/users/vcmohan/DeJitter/DeJitter.htm\n",
    "- chroma issues https://forum.videohelp.com/threads/397928-Imrpove-old-video-recorded-by-a-bad-camera?s=0a1230911434e7442d05b6b6cee8e6d2\n",
    "\n",
    "- jpeg compression artifacts\n",
    "\n",
    "## use images of related material to train superres\n",
    "- family photos to train for family videos\n",
    "- need water and outdoors\n",
    "- use video samples for video\n",
    "- use film examples for super8\n",
    "\n",
    "## Data Augmentation\n",
    "- sample a few frames from a few places\n",
    "- different blur levels- complete\n",
    "- different blur levels in the same image\n",
    "- convolution with disk - complete\n",
    "- left-right flip - complete\n",
    "- contrast and brightness - complete\n",
    "- random crop\n",
    "- crop to the super8 resolution 720x480 or video 320x240\n",
    "- GAN statistics\n",
    "- predict level of blur with same network to encourage differentiation to that\n",
    "- input of a filter that detects light reflections to help infer blurring\n",
    "- try mixup\n",
    "- use a single shot classifier output as an input\n",
    "\n",
    "- see this library for blending images https://pypi.org/project/blend-modes/\n",
    "\n",
    "\n",
    "## Optimization\n",
    "- Use Adam\n",
    "- Use optimal learning rate\n",
    "- freeze layers\n",
    "- dropout\n",
    "- use fp16 - pytorch1.6 has a new library for this - complete\n",
    "\n",
    "## losses\n",
    "- try l1 loss\n",
    "- try perceptive loss vgg18\n",
    "\n",
    "## do comparisons for standard test sets\n",
    "- not super useful since they use gaussian blurring, bicubic interpolation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "project_base = \"/media/SSD/superres/\"\n",
    "results_folder = \"Results\"\n",
    "training_folder = \"video_data\"\n",
    "model_folder = \"checkpoint\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "var nb = IPython.notebook;\n",
    "var kernel = IPython.notebook.kernel;\n",
    "var command = \"NOTEBOOK_FULL_PATH = '\" + nb.base_url + nb.notebook_path + \"'\";\n",
    "kernel.execute(command);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"NOTEBOOK_FULL_PATH:\\n\", NOTEBOOK_FULL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tie a result to a set of params.  Tie a set of params to a model.  Tie a model to a notebook.\n",
    "# register the experiment to get a unique run id\n",
    "# save all the results in a repository\n",
    "# use github to do it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess video from wmv to mp4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!ffmpeg -i '/home/filip/Videos/1987 first video making muffins.wmv'  -c:v libx264 -crf 23 -c:a aac -strict -2 -q:a 100 '/media/SSD/superres/1987 first video making muffins.mp4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python /media/SSD/superres/Zooming-Slow-Mo-CVPR-2020/codes/video_to_zsm.py --video /media/SSD/superres/snip.mp4  --model /media/SSD/superres/model/xiang2020zooming.pth --output /media/SSD/superres/muffins_test.mp4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess from mpeg2 (DVD) to mp4 with antialiasing and only 120 seconds:\n",
    "#ffmpeg -i 'Old 8mm Reels_1.VOB' -t 120  -vf \"bwdif\" -c:v libx264 -crf 28 -c:a aac -b:a 128k  'Old 8mm Reels_1.mp4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To deinterlace\n",
    "https://video.stackexchange.com/questions/17396/how-to-deinterlacing-with-ffmpeg\n",
    "https://macilatthefront.blogspot.com/2017/04/deinterlacing-hd-footage-without-losing.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#%reload_ext autoreload\n",
    "#%autoreload 2\n",
    "#%matplotlib inline\n",
    "\n",
    "\n",
    "import cv2 \n",
    "import os\n",
    "import numpy as np\n",
    "import subprocess as sp\n",
    "import time\n",
    "from tqdm import tqdm, trange\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchvision.transforms import RandomHorizontalFlip,ColorJitter\n",
    "from torchvision.transforms import Compose, CenterCrop, ToTensor, Resize, Grayscale\n",
    "\n",
    "import random\n",
    "from astropy.convolution import  Gaussian2DKernel, Tophat2DKernel,AiryDisk2DKernel\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '/media/SSD/superres/pytorch-vdsr/')\n",
    "\n",
    "#from vdsr import Net\n",
    "\n",
    "import lpips #https://github.com/richzhang/PerceptualSimilarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "\n",
    "from os import listdir\n",
    "from os.path import join\n",
    "from PIL import Image, ImageFilter\n",
    "import numpy as np\n",
    "import torchvision as vision\n",
    "\n",
    "import numbers\n",
    "\n",
    "\n",
    "torch.cuda.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a data loader\n",
    "- see https://github.com/2KangHo/vdsr_pytorch/blob/master/data.py\n",
    "- see https://github.com/2KangHo/vdsr_pytorch/blob/master/data_utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "toPIL = vision.transforms.ToPILImage()\n",
    "\n",
    "\n",
    "def noisy(img, std=3.0):\n",
    "    mean = 0.0\n",
    "    gauss = np.random.normal(mean, std, (img.height, img.width, 3))\n",
    "    # noisy = np.clip(np.uint8(img + gauss), 0, 255)\n",
    "    noisy = np.uint8(img + gauss)\n",
    "    return noisy\n",
    "\n",
    "\n",
    "def is_image_file(filename):\n",
    "    return any(filename.endswith(extension) for extension in [\".png\", \".jpg\", \".jpeg\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_video_file(path, extensions = (\"mp4\",)):\n",
    "    return path.lower().endswith(extensions)\n",
    "\n",
    "def frame_sampler1(length, frame_sample_size):\n",
    "\n",
    "    # calculate middle of video and take 'frame_sample_size' frames from middle\n",
    "    middle = length // 2\n",
    "    left_length = frame_sample_size//2\n",
    "    right_length = frame_sample_size - left_length\n",
    "    left = max(0,middle - left_length)\n",
    "    right = min(length, middle + right_length)\n",
    "          \n",
    "    return list(range(left,right))\n",
    "\n",
    "\n",
    "\n",
    "def frame_sampler2(length, frame_sample_size):\n",
    "    return np.linspace(0, length, 3+min(frame_sample_size,length)).astype(int)[2:-1]\n",
    "\n",
    "\n",
    "# Make this load still photos too, and have them added with frame = 0\n",
    "def make_framelist(video_dir,frame_sample_size = 10):\n",
    "    instances = []\n",
    "    for filename in listdir(video_dir):\n",
    "        filepath = os.path.join(video_dir,filename)\n",
    "        #print(filename)\n",
    "        \n",
    "        if is_video_file(filepath):\n",
    "            # open video file\n",
    "            cap = cv2.VideoCapture(str(filepath))\n",
    "\n",
    "            # get frame count\n",
    "            length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "            cap.release()\n",
    "            \n",
    "            if frame_sample_size is not None:\n",
    "                samples = frame_sampler2(length, frame_sample_size)\n",
    "                # append fn and frame num to instances\n",
    "            else:\n",
    "                samples = range(0,length)\n",
    "                \n",
    "            for frame in samples:\n",
    "                item = {\"Filepath\":filepath,\"Type\":\"frame\",  \"Framenum\":frame}\n",
    "                instances.append(item)\n",
    "                \n",
    "        elif is_image_file(filepath):\n",
    "            # open image file\n",
    "            img = cv2.imread(filepath)\n",
    "            item = {\"Filepath\":filepath, \"Type\":\"image\"}\n",
    "            instances.append(item)\n",
    "            \n",
    "    return instances\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list of all the data\n",
    "instances = make_framelist(\"/media/SSD/superres/video_data\",1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_frame(instance):\n",
    "    \n",
    "    path = instance[\"Filepath\"]\n",
    "    \n",
    "    if instance[\"Type\"] == \"frame\":\n",
    "        \n",
    "        frame = instance[\"Framenum\"]\n",
    "        cap = cv2.VideoCapture(path)\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame-1)\n",
    "        ret, img = cap.read()\n",
    "        if ret==0:\n",
    "            print(\"Error with:\",instance)\n",
    "    elif instance[\"Type\"] == \"image\":\n",
    "        img = cv2.imread(path)\n",
    "    # convert to PIL RGB\n",
    "    im_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    return Image.fromarray(im_rgb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class RescaleCrop(object):\n",
    "\n",
    "\n",
    "    def __init__(self, crop_size ,kernel_width):\n",
    "        \n",
    "        if isinstance(crop_size, numbers.Number):\n",
    "            self.crop_size = (int(crop_size), int(crop_size))\n",
    "        else:\n",
    "            self.crop_size = crop_size \n",
    "        \n",
    "        self.kernel_width = kernel_width\n",
    "        \n",
    "\n",
    "    \n",
    "        \n",
    "\n",
    "    def __call__(self, img):\n",
    "        \n",
    "        \n",
    "        # Ideal steps\n",
    "        # Crop with proportional scale, to simulate images of different input resolutions (if thats a requirement)\n",
    "        # This crop should leave some edges of the image to allow blurring with kernel beyond final size\n",
    "        # Blur the image \n",
    "        # If different randomly picked blurring kernels are used, they should have equivalent blurring power.\n",
    "        \n",
    "        h_size,v_size = img.size\n",
    "        \n",
    "        #print(\"im size:\", img.size)\n",
    "        \n",
    "        # Resize down to a size a bit larger than final size to allow proper blurring\n",
    "        v_wanted = int(self.crop_size[0]+self.kernel_width+1)\n",
    "        h_wanted = int(self.crop_size[1]+self.kernel_width+1)\n",
    "        \n",
    "        #print(\"im wanted:\", (h_wanted, v_wanted))\n",
    "        \n",
    "        h_scale = h_wanted/h_size\n",
    "        v_scale = v_wanted/v_size\n",
    "        \n",
    "        scale = max(h_scale, v_scale)\n",
    "        \n",
    "        #print(\"scales=\",(h_scale, v_scale))\n",
    "        \n",
    "        #print(\"new size=\",(int(h_size*scale), int(v_size*scale)))\n",
    "        img = img.resize((int(h_size*scale), int(v_size*scale)))\n",
    "        \n",
    "        img = CenterCrop((v_wanted,h_wanted))(img) \n",
    "\n",
    "        \n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class DatasetFromVideoFolder(data.Dataset):\n",
    "    def __init__(self, video_dir,size,frame_sample_size=None, input_transform=None, target_transform=None, add_noise=None, noise_std=3.0, Flip_hor=False,Rand_bright_contrast=False, kernel_width=10):\n",
    "        super(DatasetFromVideoFolder, self).__init__()\n",
    "        self.video_frames = make_framelist(video_dir,frame_sample_size)\n",
    "\n",
    "        self.input_transform = input_transform\n",
    "        self.target_transform = target_transform\n",
    "        self.add_noise = add_noise\n",
    "        self.noise_std = noise_std\n",
    "        self.Flip_hor = Flip_hor\n",
    "        self.Rand_bright_contrast = Rand_bright_contrast\n",
    "        self.size = size\n",
    "        self.kernel_width = kernel_width\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        input = load_frame(self.video_frames[index])\n",
    "        \n",
    "        input = RescaleCrop(self.size,self.kernel_width )(input)\n",
    "        \n",
    "        if self.Flip_hor:\n",
    "            input = RandomHorizontalFlip()(input)\n",
    "        \n",
    "        if self.Rand_bright_contrast:\n",
    "            input = ColorJitter(brightness=.2, contrast=.2)(input)\n",
    "        \n",
    "        \n",
    "        target = input.copy()\n",
    "        if self.input_transform:\n",
    "            if self.add_noise:\n",
    "                input = noisy(input, self.noise_std)\n",
    "                input = toPIL(input)\n",
    "            input = self.input_transform(input)\n",
    "        if self.target_transform:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return input, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(Tophat2DKernel(10).array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cv_center_crop(img,output_size):\n",
    "        if isinstance(output_size, numbers.Number):\n",
    "            output_size = (int(output_size), int(output_size))\n",
    "            \n",
    "        \n",
    "        image_height = img.shape[0]\n",
    "        image_width = img.shape[1]\n",
    "        \n",
    "        crop_height, crop_width = output_size\n",
    "        crop_top = max(0,int(round((image_height - crop_height) / 2.)))\n",
    "        crop_left = max(0,int(round((image_width - crop_width) / 2.)))\n",
    "        #print(\"input:\",img.shape)\n",
    "        #print(\"output:\",output_size)\n",
    "        #print(\"crop:\",crop_top,crop_top+output_size[0],crop_left,crop_left+output_size[1])\n",
    "        return img[crop_top:crop_top+output_size[0],crop_left:crop_left+output_size[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class CropRescale(object):\n",
    "\n",
    "\n",
    "    def __init__(self, crop_size ,factor_range):\n",
    "        \n",
    "        if isinstance(crop_size, numbers.Number):\n",
    "            self.crop_size = (int(crop_size), int(crop_size))\n",
    "        else:\n",
    "            self.crop_size = crop_size \n",
    "        \n",
    "        self.factor_range = factor_range\n",
    "        \n",
    "\n",
    "    \n",
    "        \n",
    "\n",
    "    def __call__(self, img):\n",
    "        \n",
    "        rand_scale_factor = random.uniform(*self.factor_range)\n",
    "        #print(\"size of image\",img.size)\n",
    "        \n",
    "        # Ideal steps\n",
    "        # Crop with proportional scale, to simulate images of different input resolutions (if thats a requirement)\n",
    "        # This crop should leave some edges of the image to allow blurring with kernel beyond final size\n",
    "        # Blur the image \n",
    "        # If different randomly picked blurring kernels are used, they should have equivalent blurring power.\n",
    "        \n",
    "        h_size,v_size = img.size\n",
    "        \n",
    "        #print(\"im size:\", img.size)\n",
    "        \n",
    "        # Resize down to a size a bit larger than final size to allow proper blurring\n",
    "        v_wanted = int(self.crop_size[0]+2*rand_scale_factor+2)\n",
    "        h_wanted = int(self.crop_size[1]+2*rand_scale_factor+2)\n",
    "        \n",
    "        #print(\"im wanted:\", (h_wanted, v_wanted))\n",
    "        \n",
    "        h_scale = h_wanted/h_size\n",
    "        v_scale = v_wanted/v_size\n",
    "        \n",
    "        scale = max(h_scale, v_scale)\n",
    "        \n",
    "        #print(\"scales=\",(h_scale, v_scale))\n",
    "        \n",
    "        #print(\"new size=\",(int(h_size*scale), int(v_size*scale)))\n",
    "        img = img.resize((int(h_size*scale), int(v_size*scale)))\n",
    "        \n",
    "        img_cv = cv_center_crop(np.array(img),(v_wanted,h_wanted))  \n",
    "\n",
    "        \n",
    "        #Disk Blur\n",
    "        #print(max(np.array(img)), min(np.array(img)))\n",
    "        img_cv = cv2.filter2D(img_cv, -1, Tophat2DKernel(rand_scale_factor).array)\n",
    "    \n",
    "        #Center Crop\n",
    "        img_cv = cv_center_crop(img_cv,self.crop_size)\n",
    "        \n",
    "        #Jpeg compression (for adding artifacts)\n",
    "        #rand_quality_factor = random.uniform(30,90)\n",
    "        #is_success, im_buf_arr = cv2.imencode(\".jpg\", img_cv,params = [cv2.IMWRITE_JPEG_QUALITY,30])\n",
    "        #img_cv = cv2.imdecode(im_buf_arr,flags=cv2.IMREAD_COLOR)\n",
    "        \n",
    "        \n",
    "        img= Image.fromarray(img_cv)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Crappify(object):\n",
    "\n",
    "\n",
    "    def __init__(self, crop_size ,factor_range):\n",
    "        \n",
    "        if isinstance(crop_size, numbers.Number):\n",
    "            self.crop_size = (int(crop_size), int(crop_size))\n",
    "        else:\n",
    "            self.crop_size = crop_size \n",
    "        \n",
    "        self.factor_range = factor_range\n",
    "        \n",
    "\n",
    "    \n",
    "        \n",
    "\n",
    "    def __call__(self, img):\n",
    "        \n",
    "        rand_scale_factor = random.uniform(*self.factor_range)\n",
    "        \n",
    "        img_cv = np.array(img)\n",
    "\n",
    "        \n",
    "        #Disk Blur\n",
    "        #print(max(np.array(img)), min(np.array(img)))\n",
    "        k = Tophat2DKernel(int(rand_scale_factor)/2.0).array\n",
    "        k = k/k.sum()\n",
    "        \n",
    "        \n",
    "        img_cv = cv2.filter2D(img_cv, -1, k)\n",
    "    \n",
    "        #Center Crop\n",
    "        img_cv = cv_center_crop(img_cv,self.crop_size)\n",
    "        \n",
    "        #Jpeg compression (for adding artifacts)\n",
    "        rand_quality_factor = random.uniform(30,90)\n",
    "        is_success, im_buf_arr = cv2.imencode(\".jpg\", img_cv,params = [cv2.IMWRITE_JPEG_QUALITY,30])\n",
    "        img_cv = cv2.imdecode(im_buf_arr,flags=cv2.IMREAD_COLOR)\n",
    "        \n",
    "        \n",
    "        img= Image.fromarray(img_cv)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_circular_mask(h, w, slope=0.01, center=None, radius=None, scale=1.0):\n",
    "\n",
    "    if center is None: # use the middle of the image\n",
    "        center = (int(w/2), int(h/2))\n",
    "    if radius is None: # use the smallest distance between the center and image walls\n",
    "        radius = min(center[0], center[1], w-center[0], h-center[1])\n",
    "\n",
    "    Y, X = np.ogrid[:h, :w]\n",
    "    dist_from_center = np.sqrt((scale*(X - center[0]))**2 + ((Y-center[1])/scale)**2)\n",
    "\n",
    "    mask = np.minimum((np.maximum(dist_from_center-radius,0)*slope)**(0.5),1.0)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multifuz(object):\n",
    "\n",
    "\n",
    "    def __init__(self, crop_size ,kernel_range, quality_range, slope_range, radius_range, scale_range):\n",
    "        \n",
    "        if isinstance(crop_size, numbers.Number):\n",
    "            self.crop_size = (int(crop_size), int(crop_size))\n",
    "        else:\n",
    "            self.crop_size = crop_size \n",
    "        \n",
    "        self.kernel_range = kernel_range\n",
    "        self.quality_range = quality_range\n",
    "        self.slope_range = slope_range\n",
    "        self.radius_range = radius_range\n",
    "        self.scale_range = scale_range\n",
    "        \n",
    "\n",
    "    \n",
    "        \n",
    "\n",
    "    def __call__(self, img):\n",
    "        \n",
    "        img_cv = np.array(img)\n",
    "        d = img_cv .shape\n",
    "        dim = (d[1], d[0])\n",
    "        height = d[0]\n",
    "        width = d[1]\n",
    "        color = len(d)-2\n",
    "        \n",
    "        rand_kernel = random.uniform(*self.kernel_range)\n",
    "        rand_quality = random.uniform(*self.quality_range)\n",
    "        rand_slope = random.uniform(*self.slope_range)\n",
    "        rand_radius = random.uniform(*self.radius_range)*max(height,width)\n",
    "        \n",
    "\n",
    "        rand_center = (random.uniform(0,width),random.uniform(0,height))\n",
    "        \n",
    "        rand_scale = random.uniform(0.5,2)\n",
    "        #technically should adjust the scale to the crop size scale of the final image\n",
    "        \n",
    "        #blur kernel\n",
    "        \n",
    "        k = Tophat2DKernel(int(rand_kernel)/2.0).array\n",
    "        k = k/k.sum()\n",
    "        img_cv_blur = cv2.filter2D(img_cv, cv2.CV_32F, k)\n",
    "        \n",
    "        #add them with a mask\n",
    "        # Normalize the alpha mask to keep intensity between 0 and 1\n",
    "        \n",
    "        alpha = create_circular_mask(h=height, w=width, center=rand_center, radius=rand_radius, scale = rand_scale)*1.0\n",
    "        \n",
    "        if color ==1:\n",
    "            alpha=np.reshape(alpha, (height,width,-1))\n",
    "            alpha=np.broadcast_to(alpha,[height,width,3])\n",
    "        \n",
    "        #alpha = alpha.astype(float)/255\n",
    "        \n",
    "        # Multiply the foreground with the alpha matte\n",
    "        \n",
    "        img_cv_blur = cv2.multiply(img_cv_blur, alpha, dtype=cv2.CV_32F )\n",
    "        # Multiply the background with ( 1 - alpha )\n",
    "\n",
    "        \n",
    "        img_cv = cv2.multiply(img_cv, 1.0 - alpha, dtype=cv2.CV_32F)\n",
    "        # Add the masked foreground and background.\n",
    "\n",
    "        \n",
    "        img_cv = cv2.add(img_cv, img_cv_blur, dtype=cv2.CV_32F)\n",
    "        \n",
    "        img_cv = cv2.normalize(img_cv, None, 0, 255, cv2.NORM_MINMAX, cv2.CV_8UC1)\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "        #Center Crop\n",
    "        img_cv = cv_center_crop(img_cv,self.crop_size)\n",
    "        \n",
    "        #Jpeg compression (for adding artifacts)\n",
    "        rand_quality_factor = random.uniform(30,90)\n",
    "        is_success, im_buf_arr = cv2.imencode(\".jpg\", img_cv,params = [cv2.IMWRITE_JPEG_QUALITY,30])\n",
    "        img_cv = cv2.imdecode(im_buf_arr,flags=cv2.IMREAD_COLOR)\n",
    "        \n",
    "        \n",
    "        img= Image.fromarray(img_cv)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im=load_frame(instances[10])\n",
    "im = Grayscale(num_output_channels=1)(im)\n",
    "img_cv = np.array(im)\n",
    "\n",
    "alpha = create_circular_mask(h=720, w=1280, center=None, radius=None)*1.0\n",
    "#a3 = np.broadcast_to(alpha,[720,1280,3])\n",
    "a3=np.reshape(alpha, (720,1280,-1))\n",
    "a3=np.broadcast_to(a3,[720,1280,3])\n",
    "print(\"alpha size:\", a3.shape)\n",
    "print(\"image size:\", img_cv.shape)\n",
    "res = cv2.multiply(img_cv, a3, dtype=cv2.CV_8UC1)\n",
    "\n",
    "img= Image.fromarray(res)\n",
    "img\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a3=np.reshape(alpha, (720,1280,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.broadcast_to(a3,[720,1280,3]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(img_cv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test the CropRescale transform\n",
    "im=load_frame(instances[10])\n",
    "#im = Grayscale(num_output_channels=1)(im)\n",
    "#im_res = RescaleCrop(crop_size = (480,720), kernel_width=1)(im)\n",
    "im_res = Multifuz(crop_size = (720,1280),\n",
    "                  kernel_range=(4,16), \n",
    "                  quality_range=(2,2), \n",
    "                  slope_range=(0.00001, 0.001),\n",
    "                 radius_range=(.05,.25),\n",
    "                 scale_range=(1/2,2))(im)\n",
    "print(im.size,im_res.size)\n",
    "type(im_res)\n",
    "im_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Tophat2DKernel(2).array\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances[19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_gt = Image.open(\"/media/SSD/superres/pytorch-vdsr/Set5/butterfly_GT.bmp\").convert(\"RGB\")\n",
    "#im_gt = Image.open(\"/home/filip/Pictures/Screenshot from CADDX000009.MP4.png\").convert(\"RGB\")\n",
    "im_res = CropRescale(crop_size = 256, factor_range=(4,4))(im_gt)\n",
    "print(im_gt.size,im_res.size)\n",
    "type(im_res)\n",
    "im_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def input_transform(crop_size, kernel_range):\n",
    "    \n",
    "    \n",
    "    return Compose([\n",
    "        \n",
    "        Multifuz(crop_size = crop_size,\n",
    "                  kernel_range=kernel_range, \n",
    "                  quality_range=(30,90), \n",
    "                  slope_range=(0.00001, 0.001),\n",
    "                 radius_range=(.05,.25),\n",
    "                 scale_range=(1/2,2)), \n",
    "        Grayscale(num_output_channels=1),\n",
    "        ToTensor(),\n",
    "    ])\n",
    "\n",
    "\n",
    "\n",
    "def target_transform(crop_size):\n",
    "    print(crop_size)\n",
    "    return Compose([\n",
    "        Grayscale(num_output_channels=1),\n",
    "        CenterCrop(crop_size),\n",
    "        ToTensor(),\n",
    "    ])\n",
    "\n",
    "\n",
    "def get_training_set(data_dir, crop_size, kernel_range, frame_sample_size=10, add_noise=None, \n",
    "                     noise_std=3.0,Flip_hor=True,Rand_bright_contrast=True):\n",
    "    \n",
    "\n",
    "    cropsize = crop_size\n",
    "    return DatasetFromVideoFolder(data_dir,crop_size, frame_sample_size,\n",
    "                             input_transform=input_transform(\n",
    "                                 crop_size, kernel_range),\n",
    "                             target_transform=target_transform(crop_size),\n",
    "                             add_noise=add_noise,\n",
    "                             noise_std=noise_std,Flip_hor=Flip_hor,\n",
    "                             Rand_bright_contrast=Rand_bright_contrast)\n",
    "\n",
    "\n",
    "## Below functions need work\n",
    "\n",
    "def get_validation_set(data_dir, crop_size, kernel_range):\n",
    "    \n",
    "    \n",
    "    return DatasetFromVideoFolder(data_dir,frame_sample_size,\n",
    "                             input_transform=input_transform(\n",
    "                                 cropsize, kernel_range,frame_sample_size=10),\n",
    "                             target_transform=target_transform(cropsize))\n",
    "\n",
    "\n",
    "def get_test_set(data_dir, crop_size, upscale_factor):\n",
    "\n",
    "\n",
    "    return DatasetFromVideoFolder(data_dir,frame_sample_size=None,\n",
    "                             input_transform=input_transform(\n",
    "                                 cropsize, upscale_factor),\n",
    "                             target_transform=target_transform(cropsize))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = \"/media/SSD/superres/video_data\"\n",
    "frame_sample_size =10\n",
    "batch_size = 4 # Make 4 For FP16, 480x720 size images\n",
    "crop_size = (480,720)\n",
    "kernel_range=(4,16)\n",
    "add_noise=False\n",
    "noise_std=3.0\n",
    "Flip_hor=True\n",
    "Rand_bright_contrast=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a data set\n",
    "# split it using random split\n",
    "# random_split(range(10), [3, 7], generator=torch.Generator().manual_seed(42))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = get_training_set(data_dir=train_dir, crop_size=crop_size,\n",
    "            kernel_range=kernel_range,frame_sample_size =frame_sample_size, \n",
    "            add_noise=add_noise, noise_std=noise_std,Flip_hor=Flip_hor,Rand_bright_contrast=Rand_bright_contrast)\n",
    "training_data_loader = DataLoader(dataset=train_set, num_workers=0, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# smaller and faster\n",
    "crop_size=(480//8,720//8)\n",
    "batch_size = 4*16\n",
    "train_set = get_training_set(data_dir=train_dir, crop_size=crop_size,\n",
    "            kernel_range=kernel_range,frame_sample_size =frame_sample_size, \n",
    "            add_noise=add_noise, noise_std=noise_std,Flip_hor=Flip_hor,Rand_bright_contrast=Rand_bright_contrast)\n",
    "training_data_loader = DataLoader(dataset=train_set, num_workers=0, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# measurement of blur of an image\n",
    "def variance_of_laplacian(image):\n",
    "    # compute the Laplacian of the image and then return the focus\n",
    "    # measure, which is simply the variance of the Laplacian\n",
    "    return cv2.Laplacian(image, cv2.CV_64F).var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def show_batch(dataloader,size = 8):\n",
    "\n",
    "    inputs, outputs = next(iter(dataloader))\n",
    "    inputs = inputs.numpy().transpose((0, 2, 3, 1))\n",
    "    outputs = outputs.numpy().transpose((0, 2, 3, 1))\n",
    "    \n",
    "    \n",
    "    #mean = np.array([0.485, 0.456, 0.406])\n",
    "    #std = np.array([0.229, 0.224, 0.225])\n",
    "    #inputs = inputs*std + mean\n",
    "    #outputs = outputs*std + mean\n",
    "    w,h = 2*size, 4*size\n",
    "    fig=plt.figure(figsize=(w, h))\n",
    "    columns = 2\n",
    "    rows = 4\n",
    "    ax=[]\n",
    "    \n",
    "    for i in range(0, rows):\n",
    "        blur_measure_input = variance_of_laplacian((inputs[i]*255).astype(np.uint8))\n",
    "        blur_measure_output = variance_of_laplacian((outputs[i]*255).astype(np.uint8))\n",
    "        ax.append(fig.add_subplot(rows, columns, 2*i+1))\n",
    "        ax[-1].set_title('Blur: '+str(blur_measure_input))\n",
    "        plt.imshow((inputs[i]*255).astype(np.uint8),cmap='gray')\n",
    "        ax.append(fig.add_subplot(rows, columns, 2*i+2))\n",
    "        ax[-1].set_title('Blur: '+str(blur_measure_output))\n",
    "        plt.imshow((outputs[i]*255).astype(np.uint8),cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_batch(training_data_loader,size =8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VDSR with dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from math import sqrt\n",
    "\n",
    "class Conv_ReLU_Block(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Conv_ReLU_Block, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.do = nn.Dropout2d(p=0.5, inplace=True)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.relu(self.conv(x))\n",
    "        \n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.residual_layer = self.make_layer(Conv_ReLU_Block, 18)\n",
    "        self.input = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.output = nn.Conv2d(in_channels=64, out_channels=1, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "    \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, sqrt(2. / n))\n",
    "                \n",
    "    def make_layer(self, block, num_of_layer):\n",
    "        layers = []\n",
    "        for _ in range(num_of_layer):\n",
    "            layers.append(block())\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.relu(self.input(x))\n",
    "        out = self.residual_layer(out)\n",
    "        out = self.output(out)\n",
    "        out = torch.add(out,residual)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()\n",
    "model.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"/media/SSD/superres/pytorch-vdsr/model/model_epoch_50.pth\")[\"model\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to try this\n",
    "# from https://github.com/xiezw5/Component-Divide-and-Conquer-for-Real-World-Image-Super-Resolution/blob/master/CDC/modules/loss.py\n",
    "\n",
    "class GradientPenaltyLoss(nn.Module):\n",
    "    def __init__(self, device=torch.device('cpu')):\n",
    "        super(GradientPenaltyLoss, self).__init__()\n",
    "        self.register_buffer('grad_outputs', torch.Tensor())\n",
    "        self.grad_outputs = self.grad_outputs.to(device)\n",
    "\n",
    "    def get_grad_outputs(self, input):\n",
    "        if self.grad_outputs.size() != input.size():\n",
    "            self.grad_outputs.resize_(input.size()).fill_(1.0)\n",
    "        return self.grad_outputs\n",
    "\n",
    "    def forward(self, interp, interp_crit):\n",
    "        grad_outputs = self.get_grad_outputs(interp_crit)\n",
    "        grad_interp = torch.autograd.grad(outputs=interp_crit, inputs=interp, \\\n",
    "            grad_outputs=grad_outputs, create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
    "        grad_interp = grad_interp.view(grad_interp.size(0), -1)\n",
    "        grad_interp_norm = grad_interp.norm(2, dim=1)\n",
    "\n",
    "        loss = ((grad_interp_norm - 1)**2).mean()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#criterion = nn.MSELoss(reduction = 'sum')\n",
    "#criterion = nn.L1Loss(reduction = 'sum')\n",
    "\n",
    "#loss_fn_alex = lpips.LPIPS(net='alex') # best forward scores\n",
    "#loss_fn_vgg = lpips.LPIPS(net='vgg') # closer to \"traditional\" perceptual loss, when used for optimization\n",
    "\n",
    "loss_fn = lpips.LPIPS(net='alex')\n",
    "loss_fn.cuda()\n",
    "\n",
    "def loss_scaler(im):\n",
    "    # scale to -1,1 and convert to 3 channel (greyscale to RBG)\n",
    "    return (im * 2 - 1).expand(-1, 3, -1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class P_Loss(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(P_Loss,self).__init__()\n",
    "        \n",
    "    def forward(self,x,y):\n",
    "        \n",
    "        totloss = torch.mean(loss_fn.forward(loss_scaler(x),loss_scaler(y)))\n",
    "        return totloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = P_Loss()\n",
    "#criterion.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_lr = 0.1/100\n",
    "lr_step = 3\n",
    "gradient_clip = 0.4\n",
    "epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer = optim.SGD(model.parameters(), lr=init_lr, momentum=0.9, weight_decay=1e-4)\n",
    "optimizer = optim.Adam(model.parameters(), lr=init_lr, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = optim.lr_scheduler.OneCycleLR(optimizer, \n",
    "                                          max_lr=0.01,\n",
    "                                          steps_per_epoch=len(training_data_loader),\n",
    "                                          epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 10 epochs\"\"\"\n",
    "    lr = init_lr * (0.1 ** (.5*(epoch // lr_step)))\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, epoch):\n",
    "    model_out_path = \"checkpoint/\" + \"percepmodel4_epoch_{}.pth\".format(epoch)\n",
    "    state = {\"epoch\": epoch ,\"model\": model}\n",
    "    if not os.path.exists(\"checkpoint/\"):\n",
    "        os.makedirs(\"checkpoint/\")\n",
    "\n",
    "    torch.save(state, model_out_path)\n",
    "\n",
    "    print(\"Checkpoint saved to {}\".format(model_out_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(training_data_loader, optimizer, model, criterion, epoch):\n",
    "    lr = adjust_learning_rate(optimizer, epoch-1)\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group[\"lr\"] = lr\n",
    "\n",
    "    print(\"Epoch = {}, lr = {}\".format(epoch, optimizer.param_groups[0][\"lr\"]))\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for iteration, batch in enumerate(training_data_loader, 1):\n",
    "        input, target = Variable(batch[0]), Variable(batch[1], requires_grad=False)\n",
    "\n",
    "        \n",
    "        input = input.cuda()\n",
    "        target = target.cuda()\n",
    "\n",
    "        loss = criterion(model(input), target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward() \n",
    "        nn.utils.clip_grad_norm_(model.parameters(),gradient_clip) \n",
    "        optimizer.step()\n",
    "\n",
    "        if iteration%10 == 0:\n",
    "            dt = str(datetime.datetime.now())\n",
    "            print(\"===> Time: {} Epoch[{}]({}/{}): Loss: {:.10f}\".format(dt,epoch, iteration, len(training_data_loader), loss.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from torch.cuda.amp import autocast\n",
    "from torch.cuda.amp import GradScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainfp16(training_data_loader, optimizer, scheduler, model, criterion, epoch,scaler):\n",
    "    lr = adjust_learning_rate(optimizer, epoch-1)\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group[\"lr\"] = lr\n",
    "\n",
    "    print(\"Epoch = {}, lr = {}\".format(epoch, optimizer.param_groups[0][\"lr\"]))\n",
    "\n",
    "    model.train()\n",
    "    cum_loss2=0\n",
    "    cum_loss = 0\n",
    "    ct=0\n",
    "    ct2=0\n",
    "\n",
    "    for iteration, batch in enumerate(training_data_loader, 1):\n",
    "        input, target = Variable(batch[0]), Variable(batch[1], requires_grad=False)\n",
    "\n",
    "        \n",
    "        input = input.cuda()\n",
    "        target = target.cuda()\n",
    "        \n",
    "        # Runs the forward pass with autocasting.\n",
    "        with autocast():\n",
    "            loss = criterion(model(input), target)\n",
    "            \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        #loss.backward() \n",
    "        \n",
    "        # Unscales the gradients of optimizer's assigned params in-place\n",
    "        scaler.unscale_(optimizer)\n",
    "        \n",
    "        nn.utils.clip_grad_norm_(model.parameters(),gradient_clip) \n",
    "        \n",
    "        #optimizer.step()\n",
    "        scaler.step(optimizer)\n",
    "        \n",
    "        # Updates the scale for next iteration.\n",
    "        scaler.update()\n",
    "        \n",
    "        #scheduler.step()\n",
    "        \n",
    "        cum_loss = cum_loss + loss.data\n",
    "        cum_loss2 = cum_loss2 + loss.data\n",
    "        ct2=ct2+batch_size\n",
    "        ct = ct +batch_size\n",
    "        \n",
    "\n",
    "        if iteration%10 == 0:\n",
    "            dt = str(datetime.datetime.now())\n",
    "            print(\"===> Time: {} Epoch[{}]({}/{}): LR: {} : Loss: {:.10f}\".format(dt,epoch, iteration, len(training_data_loader), optimizer.param_groups[0][\"lr\"], cum_loss/ct))\n",
    "            ct=0\n",
    "            cum_loss =0\n",
    "    print(\"Epoch Loss:\", cum_loss2/ct2)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a GradScaler once at the beginning of training for mixed precision\n",
    "scaler = GradScaler()\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "        trainfp16(training_data_loader, optimizer, scheduler, model, criterion, epoch,scaler)\n",
    "        save_checkpoint(model, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(21, 25):\n",
    "        trainfp16(training_data_loader, optimizer, scheduler, model, criterion, epoch,scaler)\n",
    "        save_checkpoint(model, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FP32 (Normal) Precision\n",
    "\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "        train(training_data_loader, optimizer, model, criterion, epoch)\n",
    "        save_checkpoint(model, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function that takes in a .mp4 video file and applies intrinsic matrix and distortion coefficients to undistort a video.  It also preserves the sound. it uses ffmpeg for some processing as well as opencv cv2 library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the colorization function\n",
    "# We'll reuse the Cb and Cr channels from bicubic interpolation\n",
    "def colorize_cv(y, ycbcr): \n",
    "    img = np.zeros((y.shape[0], y.shape[1], 3), np.uint8)\n",
    "    img[:,:,0] = y\n",
    "    img[:,:,1] = ycbcr[:,:,1]\n",
    "    img[:,:,2] = ycbcr[:,:,2]\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_YCrCb2BGR)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.cuda.amp import autocast\n",
    "def superres_mp4(infile, outfile, model, factor=1.0,display=True,bitrate= \"12000k\",errorfile = None):\n",
    "    \n",
    "    model = model.cuda()\n",
    "    #torch.set_grad_enabled(False)\n",
    "    #model.eval()\n",
    "    \n",
    "    cap = cv2.VideoCapture(str(infile))\n",
    "    \n",
    "    length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    width  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps    = cap.get(cv2.CAP_PROP_FPS)\n",
    "    \n",
    "    success_flag = False\n",
    "    \n",
    "    print(\"Original File:\", str(infile))\n",
    "    print(\"frames=\",length,\"\\nwidth=\",width,\"\\nheight=\",height,\"\\nfps=\",fps)\n",
    "    \n",
    "    \n",
    "\n",
    "    new_height = int(height*factor)\n",
    "    new_width = int(width*factor)\n",
    "    \n",
    "    print(\"\\nProcess File:\", str(outfile))\n",
    "    print(\"factor:\",factor,\"\\nwidth=\",new_width, \"\\nheight=\",new_height,\"\\nbitrate=\",bitrate)\n",
    "    \n",
    "    \n",
    "\n",
    "    dimension = '{}x{}'.format(new_width, new_height)  #ffmpeg uses bicubic as default scaling alg\n",
    "    f_format = 'bgr24' # remember OpenCV uses bgr format\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    command = ['ffmpeg',\n",
    "            '-y',\n",
    "            '-f', 'rawvideo',\n",
    "            '-vcodec','rawvideo',\n",
    "            '-s', dimension,\n",
    "            '-pix_fmt', 'bgr24',\n",
    "            '-r', str(fps),\n",
    "            '-i', '-',\n",
    "            '-i', str(infile),\n",
    "            '-c:v', 'h264',\n",
    "            '-c:a', 'aac',\n",
    "\n",
    "            '-map','0:v:0',\n",
    "            '-map','1:a:0',\n",
    "            '-shortest',\n",
    "            '-b:v', bitrate, \n",
    "            str(outfile) ]\n",
    "\n",
    "\n",
    "    if errorfile is not None:\n",
    "        ef = open(error_file,\"w+\")\n",
    "        p = sp.Popen(command, stdin=sp.PIPE, stderr=ef)\n",
    "    else:\n",
    "        p = sp.Popen(command, stdin=sp.PIPE)\n",
    "\n",
    "    # Full processing with a stream instead of a temp file for video\n",
    "    pbar = tqdm(total=length)\n",
    "    while(cap.isOpened()):\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Capture frame-by-frame\n",
    "        ret, frame = cap.read()\n",
    "        if ret == True:\n",
    "            \n",
    "            if (factor != 1.0):\n",
    "                frame = cv2.resize(frame, (new_width, new_height), interpolation=cv2.INTER_CUBIC)\n",
    "            im_b_ycbcr = cv2.cvtColor(frame, cv2.COLOR_BGR2YCR_CB)\n",
    "            im_b_y = im_b_ycbcr[:,:,0].astype(float)\n",
    "            im_input = im_b_y/255.\n",
    "            im_input = Variable(torch.from_numpy(im_input).float()).view(1, -1, im_input.shape[0], im_input.shape[1])\n",
    "            im_input = im_input.cuda()\n",
    "            \n",
    "            with autocast():\n",
    "                out = model(im_input)\n",
    "\n",
    "            out = out.cpu()\n",
    "            im_h_y = out.data[0].numpy().astype(np.float32)\n",
    "            im_h_y = im_h_y * 255.\n",
    "            im_h_y[im_h_y < 0] = 0\n",
    "            im_h_y[im_h_y > 255.] = 255.\n",
    "            im_h_y = im_h_y[0,:,:]\n",
    "\n",
    "            im_h = colorize_cv(im_h_y, im_b_ycbcr)\n",
    "            \n",
    "                \n",
    "\n",
    "            p.stdin.write(im_h.tobytes())\n",
    "\n",
    "\n",
    "            if display:\n",
    "                cv2.imshow('Processed',im_h)\n",
    "                time.sleep(10)\n",
    "                #cv2.imshow('Orig',frame)\n",
    "            pbar.update(1)\n",
    "            # Press Q on keyboard to  exit\n",
    "            if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "                success_flag = False\n",
    "                break\n",
    "        # Break the loop\n",
    "        else:\n",
    "            success_flag = True\n",
    "            break\n",
    "    # When everything done, release the video capture object\n",
    "\n",
    "    pbar.close()\n",
    "\n",
    "    p.stdin.close()\n",
    "    p.wait()\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    # Closes all the frames\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "    return success_flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the pretrained model\n",
    "\n",
    "model = torch.load(\"/media/SSD/superres/checkpoint/percepmodel3_epoch_24.pth\")[\"model\"]\n",
    "#model = torch.load(\"/media/SSD/superres/checkpoint/model_epoch_19.pth\")[\"model\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compare to benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is the function for PSNR calculation\n",
    "def PSNR(pred, gt, shave_border=0):\n",
    "    height, width = pred.shape[:2]\n",
    "    pred = pred[shave_border:height - shave_border, shave_border:width - shave_border]\n",
    "    gt = gt[shave_border:height - shave_border, shave_border:width - shave_border]\n",
    "    imdff = pred - gt\n",
    "    rmse = math.sqrt(np.mean(imdff ** 2))\n",
    "    if rmse == 0:\n",
    "        return 100\n",
    "    return 20 * math.log10(255.0 / rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the colorization function\n",
    "# We'll reuse the Cb and Cr channels from bicubic interpolation\n",
    "def colorize(y, ycbcr): \n",
    "    img = np.zeros((y.shape[0], y.shape[1], 3), np.uint8)\n",
    "    img[:,:,0] = y\n",
    "    img[:,:,1] = ycbcr[:,:,1]\n",
    "    img[:,:,2] = ycbcr[:,:,2]\n",
    "    img = Image.fromarray(img, \"YCbCr\").convert(\"RGB\")\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_PSNR(im_gt, im_b, model):\n",
    "    # Convert the images into YCbCr mode and extraction the Y channel (for PSNR calculation)\n",
    "    im_gt_ycbcr = np.array(im_gt.convert(\"YCbCr\"))\n",
    "    im_b_ycbcr = np.array(im_b.convert(\"YCbCr\"))\n",
    "    im_gt_y = im_gt_ycbcr[:,:,0].astype(float)\n",
    "    im_b_y = im_b_ycbcr[:,:,0].astype(float)\n",
    "    \n",
    "    psnr_bicubic = PSNR(im_gt_y, im_b_y)\n",
    "    print('psnr for bicubic is {}dB'.format(psnr_bicubic))\n",
    "    \n",
    "    # Prepare for the input, a pytorch tensor\n",
    "    im_input = im_b_y/255.\n",
    "    im_input = Variable(torch.from_numpy(im_input).float()).\\\n",
    "    view(1, -1, im_input.shape[0], im_input.shape[1])\n",
    "    im_input = im_input.cuda()\n",
    "    \n",
    "    with autocast():\n",
    "        out = model(im_input)\n",
    "    \n",
    "    out = out.cpu()\n",
    "    im_h_y = out.data[0].numpy().astype(np.float32)\n",
    "    im_h_y = im_h_y * 255.\n",
    "    im_h_y[im_h_y < 0] = 0\n",
    "    im_h_y[im_h_y > 255.] = 255.\n",
    "    im_h_y = im_h_y[0,:,:]\n",
    "\n",
    "\n",
    "    \n",
    "    # Calculate the PNSR for vdsr prediction\n",
    "    psnr_predicted = PSNR(im_gt_y, im_h_y)\n",
    "    print('psnr for vdsr is {}dB'.format(psnr_predicted))\n",
    "    \n",
    "    # Calculate the PNSR different between bicubic interpolation and vdsr prediction\n",
    "    print(\"PSNR improvement is {}dB\".format(psnr_predicted - psnr_bicubic))\n",
    "    \n",
    "    \n",
    "    blur_measure_gt = variance_of_laplacian((im_gt_y).astype(np.uint8))\n",
    "    blur_measure_input = variance_of_laplacian((im_b_y).astype(np.uint8))\n",
    "    blur_measure_vdsr = variance_of_laplacian((im_h_y).astype(np.uint8))\n",
    "    \n",
    "    print(\"Sharpness Measurement GroundTruth:\",blur_measure_gt)\n",
    "    print(\"Sharpness Measurement Input:\",blur_measure_input)\n",
    "    print(\"Sharpness Measurement VDSR:\",blur_measure_vdsr)\n",
    "    \n",
    "    # Colorize the grey-level image and convert into RGB mode\n",
    "    im_h = colorize(im_h_y, im_b_ycbcr)\n",
    "    im_gt = Image.fromarray(im_gt_ycbcr, \"YCbCr\").convert(\"RGB\")\n",
    "    im_b = Image.fromarray(im_b_ycbcr, \"YCbCr\").convert(\"RGB\")\n",
    "    \n",
    "    \n",
    "    # Result visualization\n",
    "    fig = plt.figure(figsize=(18, 16), dpi= 80)\n",
    "    ax = plt.subplot(131)\n",
    "    ax.imshow(im_gt)\n",
    "    ax.set_title(\"GT\")\n",
    "\n",
    "    ax = plt.subplot(132)\n",
    "    ax.imshow(im_b)\n",
    "    ax.set_title(\"Input(bicubic)\")\n",
    "\n",
    "    ax = plt.subplot(133)\n",
    "    ax.imshow(im_h)\n",
    "    ax.set_title(\"Output(vdsr)\")\n",
    "    plt.show()\n",
    "    \n",
    "    return im_h, psnr_bicubic, psnr_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the groundtruth image and the low-resolution image (downscaled with a factor of 4)\n",
    "im_gt = Image.open(\"/media/SSD/superres/pytorch-vdsr/Set5/butterfly_GT.bmp\").convert(\"RGB\")\n",
    "im_b = Image.open(\"/media/SSD/superres/pytorch-vdsr/Set5/butterfly_GT_scale_4.bmp\").convert(\"RGB\")\n",
    "im_b2 = CropRescale(crop_size = 256, factor_range=(3,3))(im_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_PSNR(im_gt, im_b2, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# convert sample videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"/media/SSD/superres/Results/Superresolution train vdsr with perception loss/\"\n",
    "\n",
    "infile = \"/media/SSD/superres/muffins30sec.mp4\"\n",
    "outfile = folder + \"muffins30secpercepmodel4-16.mp4\"\n",
    "superres_mp4(infile, outfile, model, factor=1.6,display=False,bitrate= \"4000k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = \"/media/SSD/superres/muffins30sec.mp4\"\n",
    "outfile = folder + \"muffins30secpercepmodel4-10.mp4\"\n",
    "superres_mp4(infile, outfile, model, factor=1.0,display=False,bitrate= \"4000k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "infile = \"/media/SSD/superres/muffins30sec.mp4\"\n",
    "outfile = folder + \"muffins30secpercepmodel4-20.mp4\"\n",
    "superres_mp4(infile, outfile, model, factor=2.0,display=False,bitrate= \"4000k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = \"/media/SSD/superres/muffins30sec.mp4\"\n",
    "outfile = folder + \"muffins30secpercepmodel4-30.mp4\"\n",
    "superres_mp4(infile, outfile, model, factor=3.0,display=False,bitrate= \"4000k\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dualfile= folder + \"muffins30secpercepmodel4-30dual.mp4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# double scale for video x3 2 min\n",
    "!ffmpeg -i {'\"'+infile+'\"'} -i {'\"'+outfile+'\"'} -t 120 -filter_complex \"[0:v] scale=iw*2:ih*2, pad=2*iw:ih [left]; [1:v] scale=iw/1.5:ih/1.5 [right]; [left][right] overlay=main_w/2:0\" -b:v 4000k {'\"'+dualfile+'\"'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original scale , for video x3\n",
    "#!ffmpeg -i {'\"'+infile+'\"'} -i {'\"'+outfile+'\"'} -filter_complex \"[0:v] scale=iw*1:ih*1, pad=2*iw:ih [left]; [1:v] scale=iw/3:ih/3 [right]; [left][right] overlay=main_w/2:0\" -b:v 4000k {'\"'+dualfile+'\"'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = \"/media/SSD/superres/super8/super8_best_from_mp2.mp4\"\n",
    "outfile = folder + \"super8percepmodel4.mp4\"\n",
    "error_file = \"/media/SSD/superres/error.txt\"\n",
    "superres_mp4(infile, outfile, model, factor=1.0,display=False,bitrate= \"4000k\",errorfile = error_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dualfile= folder + \"super8percepmodel4dual.mp4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original scale , for film x1, removed un needed scaling\n",
    "!ffmpeg -i {'\"'+infile+'\"'} -i {'\"'+outfile+'\"'} -filter_complex \"[0:v]  pad=2*iw:ih [left];  [left][1:v] overlay=main_w/2:0\" -b:v 4000k {'\"'+dualfile+'\"'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare this model to the last one\n",
    "infile = folder + \"super8percepmodel3.mp4\"\n",
    "outfile = folder + \"super8percepmodel4.mp4\"\n",
    "dualfile= folder + \"super8percepmodel4-tolast-dual.mp4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original scale , for film x1, removed un needed scaling\n",
    "!ffmpeg -i {'\"'+infile+'\"'} -i {'\"'+outfile+'\"'} -filter_complex \"[0:v]  pad=2*iw:ih [left];  [left][1:v] overlay=main_w/2:0\" -b:v 4000k {'\"'+dualfile+'\"'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = \"/media/SSD/superres/super8/Old 8mm Reels_1.mp4\"\n",
    "outfile = folder + \"super8longpercepmodel4.mp4\"\n",
    "error_file = \"/media/SSD/superres/error.txt\"\n",
    "superres_mp4(infile, outfile, model, factor=1.0,display=False,bitrate= \"4000k\",errorfile = error_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dualfile= folder + \"super8longpercepmodel4dual.mp4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original scale , for film x1, removed un needed scaling\n",
    "!ffmpeg -i {'\"'+infile+'\"'} -i {'\"'+outfile+'\"'} -filter_complex \"[0:v]  pad=2*iw:ih [left];  [left][1:v] overlay=main_w/2:0\" -b:v 4000k {'\"'+dualfile+'\"'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dual_superres_mp4(infile, outfile, dualfile, model, factor=1.0,errorfile = error_file):\n",
    "    \n",
    "    superres_mp4(infile, outfile, model, factor=factor,display=False,bitrate=\"4000k\",errorfile = error_file)\n",
    "    \n",
    "    # original scale , for film x1, removed un needed scaling\n",
    "    !ffmpeg -i {'\"'+infile+'\"'} -i {'\"'+outfile+'\"'} -filter_complex \"[0:v]  pad=2*iw:ih [left];  [left][1:v] overlay=main_w/2:0\" -b:v 4000k {'\"'+dualfile+'\"'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = folder + \"super8percepmodel2.mp4\"\n",
    "outfile = folder + \"super8percepmodel2x2.mp4\"\n",
    "dualfile= folder + \"super8percepmodel2x2x1dual.mp4\"\n",
    "error_file = \"/media/SSD/superres/error.txt\"\n",
    "\n",
    "dual_superres_mp4(infile, outfile, dualfile, model, factor=1.0,errorfile = error_file)\n",
    "\n",
    "infile = \"/media/SSD/superres/super8/super8_best_from_mp2.mp4\"\n",
    "dualfile= folder + \"super8percepmodel2x2origdual.mp4\"\n",
    "\n",
    "!ffmpeg -i {'\"'+infile+'\"'} -i {'\"'+outfile+'\"'} -filter_complex \"[0:v]  pad=2*iw:ih [left];  [left][1:v] overlay=main_w/2:0\" -b:v 4000k {'\"'+dualfile+'\"'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
