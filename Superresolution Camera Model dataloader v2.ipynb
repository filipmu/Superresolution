{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Camera model dataloader v2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "import os\n",
    "import cv2 \n",
    "import numpy as np\n",
    "import subprocess as sp\n",
    "import time\n",
    "from tqdm import tqdm, trange\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchvision.transforms import RandomHorizontalFlip,ColorJitter\n",
    "from torchvision.transforms import Compose, CenterCrop, ToTensor, Resize, Grayscale,ToPILImage\n",
    "\n",
    "import random\n",
    "#from astropy.convolution import  Gaussian2DKernel, Tophat2DKernel,AiryDisk2DKernel\n",
    "\n",
    "\n",
    "#import sys\n",
    "#sys.path.insert(0, '/media/SSD/superres/pytorch-vdsr/')\n",
    "\n",
    "#from vdsr import Net\n",
    "\n",
    "import lpips #https://github.com/richzhang/PerceptualSimilarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "\n",
    "from os import listdir\n",
    "from os.path import join\n",
    "from PIL import Image, ImageFilter\n",
    "import numpy as np\n",
    "import torchvision as vision\n",
    "\n",
    "import numbers\n",
    "\n",
    "\n",
    "torch.cuda.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fnmatch, re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_image_file(filename):\n",
    "    return any(filename.endswith(extension) for extension in [\".png\", \".jpg\", \".jpeg\", \".bmp\"])\n",
    "\n",
    "def is_video_file(path, extensions = (\"mp4\",)):\n",
    "    return path.lower().endswith(extensions)\n",
    "\n",
    "def frame_sampler1(length, frame_sample_size):\n",
    "\n",
    "    # calculate middle of video and take 'frame_sample_size' frames from middle\n",
    "    middle = length // 2\n",
    "    left_length = frame_sample_size//2\n",
    "    right_length = frame_sample_size - left_length\n",
    "    left = max(0,middle - left_length)\n",
    "    right = min(length, middle + right_length)\n",
    "          \n",
    "    return list(range(left,right))\n",
    "\n",
    "\n",
    "\n",
    "def frame_sampler2(length, frame_sample_size):\n",
    "    return np.linspace(0, length, 3+min(frame_sample_size,length)).astype(int)[2:-1]\n",
    "\n",
    "\n",
    "# Don't take first or last frame, since we will use frame t-1 and t+1 to predict t\n",
    "def frame_sampler3(length, frame_sample_size):\n",
    "    return np.linspace(1, length, 3+min(frame_sample_size,length-1)).astype(int)[2:-1]\n",
    "\n",
    "\n",
    "def make_framelist(video_dir,frame_sample_size = 10, match_exp = None):\n",
    "    return make_framelist_generic(video_dir,frame_sample_size,\n",
    "                                  match_exp, frame_sampler_fn = frame_sampler2)\n",
    "\n",
    "\n",
    "def make_framelist3(video_dir,frame_sample_size = 10, match_exp = None):\n",
    "    return make_framelist_generic(video_dir,frame_sample_size,\n",
    "                                  match_exp, frame_sampler_fn = frame_sampler3)\n",
    "\n",
    "\n",
    "# Make this load still photos too, and have them added with frame = 0\n",
    "def make_framelist_generic(video_dir,frame_sample_size = 10, match_exp = None, frame_sampler_fn = frame_sampler2):\n",
    "    instances = []\n",
    "    \n",
    "    if match_exp is not None:\n",
    "        regex = fnmatch.translate(match_exp)\n",
    "        reobj = re.compile(regex)\n",
    "    \n",
    "    \n",
    "    for filename in listdir(video_dir):\n",
    "        \n",
    "\n",
    "        #if fnmatch.fnmatch(filename, match_exp):\n",
    "        if match_exp is None or reobj.match(filename):\n",
    "            filepath = os.path.join(video_dir,filename)\n",
    "            #print(filename)\n",
    "\n",
    "            if is_video_file(filepath):\n",
    "                # open video file\n",
    "                cap = cv2.VideoCapture(str(filepath))\n",
    "\n",
    "                # get frame count\n",
    "                length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "                cap.release()\n",
    "\n",
    "                if frame_sample_size is not None:\n",
    "                    samples = frame_sampler_fn(length, frame_sample_size)\n",
    "                    # append fn and frame num to instances\n",
    "                else:\n",
    "                    samples = range(0,length)\n",
    "\n",
    "                for frame in samples:\n",
    "                    item = {\"Filepath\":filepath,\"Type\":\"frame\",  \"Framenum\":frame}\n",
    "                    instances.append(item)\n",
    "\n",
    "            elif is_image_file(filepath):\n",
    "                # open image file\n",
    "                img = cv2.imread(filepath)\n",
    "                item = {\"Filepath\":filepath, \"Type\":\"image\"}\n",
    "                instances.append(item)\n",
    "            \n",
    "    return instances\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_frame(instance):\n",
    "    \n",
    "    path = instance[\"Filepath\"]\n",
    "    \n",
    "    if instance[\"Type\"] == \"frame\":\n",
    "        \n",
    "        frame = instance[\"Framenum\"]\n",
    "        cap = cv2.VideoCapture(path)\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame-1)\n",
    "        ret, img = cap.read()\n",
    "        if ret==0:\n",
    "            print(\"Error with:\",instance)\n",
    "    elif instance[\"Type\"] == \"image\":\n",
    "        img = cv2.imread(path)\n",
    "    # convert to PIL RGB\n",
    "    im_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    return Image.fromarray(im_rgb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tophat2D(r):\n",
    "    \n",
    "    d=2*int(r)+1\n",
    "    Y, X = np.ogrid[:d, :d]\n",
    "    c=int(r)\n",
    "    k = (np.sqrt((X-c)**2 +  (Y-c)**2) <= r)*1 \n",
    "    \n",
    "    return k/k.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RescaleCrop(object):\n",
    "\n",
    "\n",
    "    def __init__(self, crop_size ,kernel_width):\n",
    "        \n",
    "        if isinstance(crop_size, numbers.Number):\n",
    "            self.crop_size = (int(crop_size), int(crop_size))\n",
    "        else:\n",
    "            self.crop_size = crop_size \n",
    "        \n",
    "        self.kernel_width = kernel_width\n",
    "        \n",
    "\n",
    "    \n",
    "        \n",
    "\n",
    "    def __call__(self, img):\n",
    "        \n",
    "        \n",
    "        # Ideal steps\n",
    "        # Crop with proportional scale, to simulate images of different input resolutions (if thats a requirement)\n",
    "        # This crop should leave some edges of the image to allow blurring with kernel beyond final size\n",
    "        # Blur the image \n",
    "        # If different randomly picked blurring kernels are used, they should have equivalent blurring power.\n",
    "        \n",
    "        h_size,v_size = img.size\n",
    "        \n",
    "        #print(\"im size:\", img.size)\n",
    "        \n",
    "        # Resize down to a size a bit larger than final size to allow proper blurring\n",
    "        v_wanted = int(self.crop_size[0]+self.kernel_width+1)\n",
    "        h_wanted = int(self.crop_size[1]+self.kernel_width+1)\n",
    "        \n",
    "        #print(\"im wanted:\", (h_wanted, v_wanted))\n",
    "        \n",
    "        h_scale = h_wanted/h_size\n",
    "        v_scale = v_wanted/v_size\n",
    "        \n",
    "        scale = max(h_scale, v_scale)\n",
    "        \n",
    "        #print(\"scales=\",(h_scale, v_scale))\n",
    "        \n",
    "        #print(\"new size=\",(int(h_size*scale), int(v_size*scale)))\n",
    "        img = img.resize((int(h_size*scale), int(v_size*scale)))\n",
    "        \n",
    "        img = CenterCrop((v_wanted,h_wanted))(img) \n",
    "\n",
    "        \n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cv_center_crop(img,output_size):\n",
    "        if isinstance(output_size, numbers.Number):\n",
    "            output_size = (int(output_size), int(output_size))\n",
    "            \n",
    "        \n",
    "        image_height = img.shape[0]\n",
    "        image_width = img.shape[1]\n",
    "        \n",
    "        crop_height, crop_width = output_size\n",
    "        crop_top = max(0,int(round((image_height - crop_height) / 2.)))\n",
    "        crop_left = max(0,int(round((image_width - crop_width) / 2.)))\n",
    "        #print(\"input:\",img.shape)\n",
    "        #print(\"output:\",output_size)\n",
    "        #print(\"crop:\",crop_top,crop_top+output_size[0],crop_left,crop_left+output_size[1])\n",
    "        return img[crop_top:crop_top+output_size[0],crop_left:crop_left+output_size[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_circular_mask(h, w, slope=0.01, center=None, radius=None, scale=1.0):\n",
    "\n",
    "    if center is None: # use the middle of the image\n",
    "        center = (int(w/2), int(h/2))\n",
    "    if radius is None: # use the smallest distance between the center and image walls\n",
    "        radius = min(center[0], center[1], w-center[0], h-center[1])\n",
    "\n",
    "    Y, X = np.ogrid[:h, :w]\n",
    "    dist_from_center = np.sqrt((scale*(X - center[0]))**2 + ((Y-center[1])/scale)**2)\n",
    "\n",
    "    mask = np.minimum((np.maximum(dist_from_center-radius,0)*slope)**(0.5),1.0)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_disk_blur(img_cv, kernel_width):\n",
    "    k = tophat2D(int(kernel_width)/2.0)\n",
    "    return cv2.filter2D(img_cv, cv2.CV_32F, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pil_disk_blur(img,kernel_width):\n",
    "    img_cv = cv_disk_blur(np.array(img),kernel_width)\n",
    "    img_cv = cv2.normalize(img_cv, None, 0, 255, cv2.NORM_MINMAX, cv2.CV_8UC1)\n",
    "    return Image.fromarray(img_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multifuz(object):\n",
    "\n",
    "\n",
    "    def __init__(self, crop_size ,kernel_a, kernel_b, quality, slope, radius, scale, center):\n",
    "        \n",
    "        if isinstance(crop_size, numbers.Number):\n",
    "            self.crop_size = (int(crop_size), int(crop_size))\n",
    "        else:\n",
    "            self.crop_size = crop_size \n",
    "        \n",
    "        self.kernel_a = kernel_a\n",
    "        self.kernel_b = kernel_b\n",
    "        self.quality = quality.item()\n",
    "        self.slope = slope\n",
    "        self.radius = radius\n",
    "        self.scale = scale\n",
    "        self.center = center\n",
    "        \n",
    "\n",
    "    \n",
    "        \n",
    "\n",
    "    def __call__(self, img):\n",
    "        \n",
    "        img_cv = np.array(img)\n",
    "        d = img_cv .shape\n",
    "        dim = (d[1], d[0])\n",
    "        height = d[0]\n",
    "        width = d[1]\n",
    "        color = len(d)-2\n",
    "        \n",
    "        \n",
    "        #blur kernel A\n",
    "        \n",
    "        k = tophat2D(int(self.kernel_a)/2.0)\n",
    "        img_cv_blurA = cv2.filter2D(img_cv, cv2.CV_32F, k)\n",
    "        img_cv_blurA = cv_center_crop(img_cv_blurA,self.crop_size)\n",
    "        \n",
    "\n",
    "        \n",
    "        #add them with a mask\n",
    "        # Normalize the alpha mask to keep intensity between 0 and 1\n",
    "        \n",
    "        if self.radius > 0:\n",
    "            \n",
    "            #blur kernel B\n",
    "            k = tophat2D(int(self.kernel_b)/2.0)\n",
    "            img_cv_blurB = cv2.filter2D(img_cv, cv2.CV_32F, k)\n",
    "            img_cv_blurB = cv_center_crop(img_cv_blurB,self.crop_size)            \n",
    "            \n",
    "            \n",
    "            alpha = create_circular_mask(h=self.crop_size[0], w=self.crop_size[1], slope=self.slope,center=self.center,\n",
    "                                         radius=self.radius, scale = self.scale)*1.0\n",
    "\n",
    "            if color ==1:\n",
    "                alpha=np.reshape(alpha, (self.crop_size[0],self.crop_size[1],-1))\n",
    "                alpha=np.broadcast_to(alpha,[self.crop_size[0],self.crop_size[1],3])\n",
    "\n",
    "            #alpha = alpha.astype(float)/255\n",
    "\n",
    "            # Multiply the foreground with the alpha matte\n",
    "\n",
    "\n",
    "\n",
    "            img_cv_blurA = cv2.multiply(img_cv_blurA, alpha, dtype=cv2.CV_32F )\n",
    "            # Multiply the background with ( 1 - alpha )\n",
    "\n",
    "\n",
    "            img_cv_blurB = cv2.multiply(img_cv_blurB, 1.0 - alpha, dtype=cv2.CV_32F)\n",
    "            # Add the masked foreground and background.\n",
    "\n",
    "            #img_cv = img_cv_blurA\n",
    "            img_cv = cv2.add(img_cv_blurA, img_cv_blurB, dtype=cv2.CV_32F)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            img_cv = img_cv_blurA\n",
    "        \n",
    "        img_cv = cv2.normalize(img_cv, None, 0, 255, cv2.NORM_MINMAX, cv2.CV_8UC1)\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "        #Center Crop\n",
    "        #img_cv = cv_center_crop(img_cv,self.crop_size)\n",
    "        \n",
    "        #Jpeg compression (for adding artifacts)\n",
    "        \n",
    "        is_success, im_buf_arr = cv2.imencode(\".jpg\", img_cv,params = [cv2.IMWRITE_JPEG_QUALITY,self.quality])\n",
    "        \n",
    "        \n",
    "        img_cv = cv2.imdecode(im_buf_arr,flags=cv2.IMREAD_COLOR)\n",
    "        \n",
    "        \n",
    "        img= Image.fromarray(img_cv)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetFromVideoFolder(data.Dataset):\n",
    "    def __init__(self, video_dir,crop_size,frame_sample_size=10,max_kernel=10,Flip_hor=True,\n",
    "                 Rand_bright_contrast=True, input_transform =None,target_transform=None, match_exp=\"*.*\", **camera_params):\n",
    "        \n",
    "        #  add_noise=None, noise_std=3.0, Flip_hor=False,Rand_bright_contrast=False, kernel_width=10\n",
    "\n",
    "        super(DatasetFromVideoFolder, self).__init__()\n",
    "        self.video_frames = make_framelist(video_dir,frame_sample_size,match_exp)\n",
    "        \n",
    "        self.Flip_hor = Flip_hor\n",
    "        self.Rand_bright_contrast = Rand_bright_contrast\n",
    "        self.crop_size = crop_size\n",
    "        self.max_kernel = max_kernel\n",
    "        self.target_transform = target_transform\n",
    "        self.camera_params = camera_params\n",
    "        self.input_transform = input_transform\n",
    "        \n",
    "    def camera_transform(self,img_hr):\n",
    "        \n",
    "        # go through the parameters and simulate the camera\n",
    "        width, height = img_hr.size\n",
    "               \n",
    "        crop_top = max(0,int(round((height - self.crop_size[0]) / 2.)))\n",
    "        crop_bottom = crop_top + self.crop_size[0]\n",
    "        crop_left = max(0,int(round((width - self.crop_size[1]) / 2.)))\n",
    "        crop_right = crop_left + self.crop_size[1]\n",
    "        \n",
    "        camera_specs = {}\n",
    "        camera_specs[\"crop_size\"] = np.int16(self.crop_size)\n",
    "        \n",
    "        camera_specs[\"center\"] = np.int16((random.uniform(crop_left, crop_right),random.uniform(crop_top, crop_bottom)))\n",
    "        \n",
    "        camera_specs[\"scale\"] = np.float32(random.uniform(0.5,2))\n",
    "       \n",
    "        \n",
    "        #camera_specs[\"kernel_a\"] = np.float32(random.uniform(*self.camera_params[\"kernel_range\"]))\n",
    "        #camera_specs[\"kernel_b\"] = np.float32(random.uniform(*self.camera_params[\"kernel_range\"]))\n",
    "        \n",
    "        camera_specs[\"kernel_a\"] = np.int16(random.uniform(*self.camera_params[\"kernel_range\"]))\n",
    "        camera_specs[\"kernel_b\"] = np.int16(random.uniform(*self.camera_params[\"kernel_range\"]))\n",
    "        camera_specs[\"quality\"] = np.int16(random.uniform(*self.camera_params[\"quality_range\"]))\n",
    "        camera_specs[\"slope\"] = np.float32(random.uniform(*self.camera_params[\"slope_range\"]))\n",
    "        camera_specs[\"radius\"] = np.float32(random.uniform(*self.camera_params[\"radius_range\"])*max(self.crop_size[0],self.crop_size[1]))\n",
    "        \n",
    "        img_lr = Multifuz(**camera_specs)(img_hr)\n",
    "        return img_lr, camera_specs\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_hr = load_frame(self.video_frames[index])\n",
    "        if self.crop_size is not None:\n",
    "            img_hr = RescaleCrop(self.crop_size,self.max_kernel )(img_hr)\n",
    "        \n",
    "        if self.Flip_hor:\n",
    "            img_hr = RandomHorizontalFlip()(img_hr)\n",
    "        \n",
    "        if self.Rand_bright_contrast:\n",
    "            img_hr = ColorJitter(brightness=.2, contrast=.2)(img_hr)\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        # go through the parameters and simulate the camera\n",
    "        img_lr,camera_specs = self.camera_transform(img_hr)\n",
    "            \n",
    "        \n",
    "        if self.input_transform:\n",
    "            img_lr = self.input_transform(img_lr)\n",
    "        \n",
    "        if self.target_transform:\n",
    "            img_hr = self.target_transform(img_hr)\n",
    "            \n",
    "        #print(camera_specs)\n",
    "        return img_lr, img_hr, camera_specs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# measurement of blur of an image\n",
    "def variance_of_laplacian(image):\n",
    "    # compute the Laplacian of the image and then return the focus\n",
    "    # measure, which is simply the variance of the Laplacian\n",
    "    return cv2.Laplacian(image, cv2.CV_64F).var()\n",
    "\n",
    "\n",
    "def show_batch(dataloader,size = 8):\n",
    "\n",
    "    inputs, outputs, camera_specs = next(iter(dataloader))\n",
    "    #print(inputs.shape)\n",
    "    inputs = inputs.numpy().transpose((0, 2, 3, 1))\n",
    "    outputs = outputs.numpy().transpose((0, 2, 3, 1))\n",
    "    #print(camera_specs)\n",
    "    \n",
    "    #mean = np.array([0.485, 0.456, 0.406])\n",
    "    #std = np.array([0.229, 0.224, 0.225])\n",
    "    #inputs = inputs*std + mean\n",
    "    #outputs = outputs*std + mean\n",
    "    w,h = 2*size, 4*size\n",
    "    fig=plt.figure(figsize=(w, h))\n",
    "    columns = 2\n",
    "    rows = 4\n",
    "    ax=[]\n",
    "    \n",
    "    for i in range(0, rows):\n",
    "        blur_measure_input = variance_of_laplacian((inputs[i]*255).astype(np.uint8))\n",
    "        blur_measure_output = variance_of_laplacian((outputs[i]*255).astype(np.uint8))\n",
    "        ax.append(fig.add_subplot(rows, columns, 2*i+1))\n",
    "        ax[-1].set_title('Blur: '+str(blur_measure_input))\n",
    "        plt.imshow((inputs[i]*255).astype(np.uint8),cmap='gray')\n",
    "        ax.append(fig.add_subplot(rows, columns, 2*i+2))\n",
    "        ax[-1].set_title('Blur: '+str(blur_measure_output))\n",
    "        plt.imshow((outputs[i]*255).astype(np.uint8),cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the colorization function\n",
    "# We'll reuse the Cb and Cr channels from bicubic interpolation\n",
    "def colorize_cv(y, ycbcr): \n",
    "    img = np.zeros((y.shape[0], y.shape[1], 3), np.uint8)\n",
    "    img[:,:,0] = y\n",
    "    img[:,:,1] = ycbcr[:,:,1]\n",
    "    img[:,:,2] = ycbcr[:,:,2]\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_YCrCb2BGR)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decolorize_cv(frame):\n",
    "    im_ycbcr = cv2.cvtColor(frame, cv2.COLOR_BGR2YCR_CB)\n",
    "    im_b_y = im_ycbcr[:,:,0].astype(float)\n",
    "    im_grey = im_b_y/255.0\n",
    "    return im_grey, im_ycbcr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv2_to_batch_tensor(img_bgr):\n",
    "    mn=np.array([0.485, 0.456, 0.406])\n",
    "    std=np.array([0.229, 0.224, 0.225])\n",
    "    im_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
    "    im_rgb = im_rgb/255.0\n",
    "    im_rgb = (im_rgb-mn)/std\n",
    "    tform = im_rgb.transpose(( 2,0, 1))\n",
    "    tform = torch.from_numpy(tform).float()\n",
    "    tform = tform.unsqueeze(0)\n",
    "    return tform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_tensor_to_cv2(bt_3ch):\n",
    "    bt_3ch = (bt_3ch*255.0).float().clamp(0,255)\n",
    "    t_3ch = bt_3ch.squeeze(0)\n",
    "    #t_3ch = t_3ch.cpu()\n",
    "    im_rbg = t_3ch.permute(1,2,0).numpy().astype(np.uint8)\n",
    "    im_bgr = cv2.cvtColor(im_rbg, cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    return im_bgr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#test\n",
    "#img_cv = cv2.imread(\"/media/SSD/superres/Screenshot from snip.mp4.png\")\n",
    "img_cv = cv2.imread(\"/home/filip/Pictures/Screenshot from sup8snip.mp4.png\")\n",
    "\n",
    "\n",
    "\n",
    "inputs = cv2_to_batch_tensor(img_bgr=img_cv)\n",
    "inputs = inputs.cuda()\n",
    "\n",
    "            \n",
    "with torch.no_grad():\n",
    "    with autocast():\n",
    "        out = mcombine(inputs)\n",
    "im_out = batch_tensor_to_cv2(out[0].cpu())\n",
    "plt.imshow(cv2.cvtColor(im_out, cv2.COLOR_BGR2RGB))\n",
    "print(\"k=\",out[1][0].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.cuda.amp import autocast\n",
    "\n",
    "def superres_mp4(infile, outfile, model,channels=1, factor=1.0,display=True,\n",
    "                 bitrate= \"12000k\",errorfile = None, k_scale = 1, center_crop = None, add_k_bar = False):\n",
    "    \n",
    "    mn=np.array([0.485, 0.456, 0.406])\n",
    "    std=np.array([0.229, 0.224, 0.225])\n",
    "    \n",
    "    model = model.cuda()\n",
    "    #torch.set_grad_enabled(False)\n",
    "    model.eval()\n",
    "    \n",
    "    cap = cv2.VideoCapture(str(infile))\n",
    "    length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    fps    = cap.get(cv2.CAP_PROP_FPS)\n",
    "    \n",
    "    if center_crop is None:\n",
    "        width  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        \n",
    "    else:\n",
    "        width,height = center_crop\n",
    "    \n",
    "    \n",
    "    success_flag = False\n",
    "    \n",
    "    print(\"Original File:\", str(infile))\n",
    "    print(\"frames=\",length,\"\\nwidth=\",width,\"\\nheight=\",height,\"\\nfps=\",fps)\n",
    "    \n",
    "    \n",
    "\n",
    "    new_height = int(height*factor)\n",
    "    new_width = int(width*factor)\n",
    "    \n",
    "    print(\"\\nProcess File:\", str(outfile))\n",
    "    print(\"factor:\",factor,\"\\nwidth=\",new_width, \"\\nheight=\",new_height,\"\\nbitrate=\",bitrate)\n",
    "    \n",
    "    \n",
    "\n",
    "    dimension = '{}x{}'.format(new_width, new_height)  #ffmpeg uses bicubic as default scaling alg\n",
    "    f_format = 'bgr24' # remember OpenCV uses bgr format\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    command = ['ffmpeg',\n",
    "            '-y',\n",
    "            '-f', 'rawvideo',\n",
    "            '-vcodec','rawvideo',\n",
    "            '-s', dimension,\n",
    "            '-pix_fmt', 'bgr24',\n",
    "            '-r', str(fps),\n",
    "            '-i', '-',\n",
    "            '-i', str(infile),\n",
    "            '-c:v', 'h264',\n",
    "            '-c:a', 'aac',\n",
    "\n",
    "            '-map','0:v:0',\n",
    "            '-map','1:a:0',\n",
    "            '-shortest',\n",
    "            '-b:v', bitrate, \n",
    "            str(outfile) ]\n",
    "\n",
    "\n",
    "    if errorfile is not None:\n",
    "        ef = open(errorfile,\"w+\")\n",
    "        p = sp.Popen(command, stdin=sp.PIPE, stderr=ef)\n",
    "    else:\n",
    "        p = sp.Popen(command, stdin=sp.PIPE)\n",
    "\n",
    "    # Full processing with a stream instead of a temp file for video\n",
    "    k_preds = []\n",
    "    pbar = tqdm(total=length)\n",
    "    while(cap.isOpened()):\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Capture frame-by-frame\n",
    "        ret, frame = cap.read()\n",
    "        if ret == True:\n",
    "            \n",
    "            #crop frame if needed\n",
    "            if center_crop is not None:\n",
    "                frame = cv_center_crop(frame,(height,width))\n",
    "            \n",
    "            if (factor != 1.0):\n",
    "                frame = cv2.resize(frame, (new_width, new_height), interpolation=cv2.INTER_LANCZOS4)\n",
    "                #frame = cv2.resize(frame, (new_width, new_height), interpolation=cv2.INTER_LINEAR)\n",
    "            \n",
    "            if channels == 1:\n",
    "                im_input, im_b_ycbcr = decolorize_cv(frame)\n",
    "\n",
    "                im_input = Variable(torch.from_numpy(im_input).float()).view(1, -1, im_input.shape[0], im_input.shape[1])\n",
    "                im_input = im_input.cuda()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    with autocast():\n",
    "                        mout = model(im_input)\n",
    "                        \n",
    "                if len(mout) >1:\n",
    "                    out=mout[0].cpu()\n",
    "                    \n",
    "                else:\n",
    "                    out = mout.cpu()\n",
    "                    \n",
    "                out = (out*255.0).clamp(0,255)\n",
    "                \n",
    "\n",
    "                im_h_y = out.data[0].numpy().astype(np.float32)\n",
    "\n",
    "                im_h_y = im_h_y[0,:,:]\n",
    "\n",
    "                im_h = colorize_cv(im_h_y, im_b_ycbcr)\n",
    "            elif channels == 3:\n",
    "                im_input = cv2_to_batch_tensor(frame)\n",
    "        \n",
    "  \n",
    "                im_input = im_input.cuda()\n",
    "    \n",
    "                \n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    with autocast():\n",
    "                        mout = model(im_input)  \n",
    "                \n",
    "                if len(mout) >1:\n",
    "                    out=mout[0].cpu()\n",
    "                    \n",
    "                else:\n",
    "                    out = mout.cpu()\n",
    "                    \n",
    "                \n",
    "                im_h = batch_tensor_to_cv2(out)\n",
    "                \n",
    "\n",
    "            #Add notation and crude bar chart to video frame\n",
    "            \n",
    "            if add_k_bar and len(mout)>1:\n",
    "                cv2.putText(im_h, text='k={:.2f}'.format(k_scale*mout[1].item()), org=(new_width-70,new_height-50),\n",
    "                fontFace= cv2.FONT_HERSHEY_SIMPLEX, fontScale=0.5, color=(0,0,255),\n",
    "                thickness=1, lineType=cv2.LINE_AA)\n",
    "                \n",
    "                cv2.putText(im_h, text='_', org=(new_width-5,new_height - int(new_height*mout[1].item()/17)),\n",
    "                fontFace= cv2.FONT_HERSHEY_SIMPLEX, fontScale=0.5, color=(0,0,255),\n",
    "                thickness=2, lineType=cv2.LINE_AA)\n",
    "                \n",
    "                \n",
    "            p.stdin.write(im_h.tobytes())\n",
    "\n",
    "\n",
    "            if display:\n",
    "                cv2.imshow('Processed',im_h)\n",
    "                time.sleep(10)\n",
    "                #cv2.imshow('Orig',frame)\n",
    "            pbar.update(1)\n",
    "            # Press Q on keyboard to  exit\n",
    "            if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "                success_flag = False\n",
    "                break\n",
    "        # Break the loop\n",
    "        else:\n",
    "            success_flag = True\n",
    "            break\n",
    "    # When everything done, release the video capture object\n",
    "\n",
    "    pbar.close()\n",
    "\n",
    "    p.stdin.close()\n",
    "    p.wait()\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    # Closes all the frames\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "    return success_flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ffmpeg_side_by_side(file_left, file_right, file_out, resize = \"Right\", bitrate = \"4000k\"):\n",
    "    \n",
    "    \n",
    "    def cv_vid_data(infile):\n",
    "        \n",
    "        vid_data = {}\n",
    "        cap = cv2.VideoCapture(str(infile))\n",
    "        vid_data[\"length\"] = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        vid_data[\"fps\"]    = cap.get(cv2.CAP_PROP_FPS)\n",
    "        vid_data[\"width\"]  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        vid_data[\"height\"] = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        cap.release()\n",
    "        \n",
    "        return vid_data\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    left_vid = cv_vid_data(file_left)\n",
    "    right_vid = cv_vid_data(file_right)\n",
    "    \n",
    "    #\"[0:v] scale=iw*2:ih*2, pad=2*iw:ih [left]; [1:v] scale=iw/1.5:ih/1.5 [right]; [left][right] overlay=main_w/2:0\"\n",
    "    \n",
    "    \n",
    "\n",
    "            \n",
    "    if resize == \"Left\":\n",
    "        \n",
    "        master_h = left_vid[\"height\"]\n",
    "        scale = master_h/right_vid[\"height\"]\n",
    "        \n",
    "        new_w = left_vid[\"width\"] + int(right_vid[\"width\"]*scale)+30\n",
    "        new_h = master_h +20\n",
    "\n",
    "        l_x = 10\n",
    "        l_y = (new_h-left_vid[\"height\"])//2\n",
    "\n",
    "        r_x = left_vid[\"width\"] + 20\n",
    "        r_y = l_y\n",
    "\n",
    "        fs = f\"[0:v] pad={new_w}:{new_h}:{l_x}:{l_y}:gray [left];\"\n",
    "        fs = fs + f\"[1:v] scale={-1}:{master_h} [right];\"\n",
    "        fs = fs + f\" [left][right] overlay={r_x}:{r_y}\"        \n",
    "\n",
    "    elif resize == \"Right\":\n",
    "        \n",
    "        master_h = right_vid[\"height\"]\n",
    "        \n",
    "        scale = master_h/left_vid[\"height\"]\n",
    "        \n",
    "        new_w = int(left_vid[\"width\"]*scale) + right_vid[\"width\"]+30\n",
    "        new_h = master_h +20\n",
    "\n",
    "        l_x = 10\n",
    "        l_y = (new_h-right_vid[\"height\"])//2\n",
    "\n",
    "        r_x = int(left_vid[\"width\"]*scale) + 20\n",
    "        r_y = l_y\n",
    "\n",
    "        fs = f\"[0:v] scale={-1}:{master_h}, pad={new_w}:{new_h}:{l_x}:{l_y}:gray [left];\"\n",
    "        fs = fs + f\" [left][1:v] overlay={r_x}:{r_y}\"  \n",
    "            \n",
    "    else:\n",
    "\n",
    "         \n",
    "        new_w = left_vid[\"width\"] + right_vid[\"width\"]+30\n",
    "        new_h = max(left_vid[\"height\"] , right_vid[\"height\"])+20\n",
    "\n",
    "        l_x = 10\n",
    "        l_y = (new_h-left_vid[\"height\"])//2\n",
    "\n",
    "        r_x = left_vid[\"width\"] + 20\n",
    "        r_y = (new_h - right_vid[\"height\"])//2\n",
    "\n",
    "        fs = f\"[0:v] pad={new_w}:{new_h}:{l_x}:{l_y}:gray [left];\"\n",
    "        fs = fs + f\" [left][1:v] overlay={r_x}:{r_y}\"\n",
    "\n",
    "\n",
    "\n",
    "    command = ['ffmpeg',\n",
    "            '-y',\n",
    "            '-i', str(file_left),\n",
    "            '-i', str(file_right),\n",
    "            '-filter_complex', fs,\n",
    "            '-b:v',bitrate, \n",
    "            str(file_out)]\n",
    "    \n",
    "    #print(command)\n",
    "    #ef = open(errorfile,\"w+\")\n",
    "    #p = sp.Popen(command, stderr=ef, shell=True)\n",
    "    sp.run(command, capture_output=True)\n",
    "    \n",
    "    #p.wait()\n",
    "    #ef.close()\n",
    "    return command"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compare to benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is the function for PSNR calculation\n",
    "def PSNR(pred, gt, shave_border=0):\n",
    "    height, width = pred.shape[:2]\n",
    "    pred = pred[shave_border:height - shave_border, shave_border:width - shave_border]\n",
    "    gt = gt[shave_border:height - shave_border, shave_border:width - shave_border]\n",
    "    imdff = pred - gt\n",
    "    rmse = math.sqrt(np.mean(imdff ** 2))\n",
    "    if rmse == 0:\n",
    "        return 100\n",
    "    return 20 * math.log10(255.0 / rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the colorization function\n",
    "# We'll reuse the Cb and Cr channels from bicubic interpolation\n",
    "def colorize(y, ycbcr): \n",
    "    img = np.zeros((y.shape[0], y.shape[1], 3), np.uint8)\n",
    "    img[:,:,0] = y\n",
    "    img[:,:,1] = ycbcr[:,:,1]\n",
    "    img[:,:,2] = ycbcr[:,:,2]\n",
    "    img = Image.fromarray(img, \"YCbCr\").convert(\"RGB\")\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decolorize(x):\n",
    "    x_ycbcr = cv2.cvtColor(x, cv2.COLOR_BGR2YCR_CB)\n",
    "    x_y = x_ycbcr[:,:,0].astype(float)\n",
    "    return x_y, x_ycbcr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_PSNR(im_gt, im_b, im_in, model,channels=1, display=True, k_scale = 17):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    #mn=np.array([0.485, 0.456, 0.406])\n",
    "    #std=np.array([0.229, 0.224, 0.225])\n",
    "    \n",
    "\n",
    "    # Convert the images into YCbCr mode and extraction the Y channel (for PSNR calculation)\n",
    "    im_gt_ycbcr = np.array(im_gt.convert(\"YCbCr\"))\n",
    "    im_b_ycbcr = np.array(im_b.convert(\"YCbCr\"))\n",
    "    im_in_ycbcr = np.array(im_in.convert(\"YCbCr\"))\n",
    "    \n",
    "    im_gt_y = im_gt_ycbcr[:,:,0].astype(float)\n",
    "    im_b_y = im_b_ycbcr[:,:,0].astype(float)\n",
    "    im_in_y = im_in_ycbcr[:,:,0].astype(float)\n",
    "    \n",
    "    #psnr_bicubic = PSNR(im_gt_y, im_b_y)\n",
    "    #psnr_input = PSNR(im_gt_y, im_in_y)\n",
    "    psnr_bicubic = PSNR(np.array(im_gt),np.array( im_b))\n",
    "    psnr_input = PSNR(np.array(im_gt), np.array(im_in))\n",
    "    \n",
    "    im_i_ycbcr = np.array(im_in.convert(\"YCbCr\"))\n",
    "    im_i_y = im_i_ycbcr[:,:,0].astype(float)\n",
    "        \n",
    "    # Prepare for the input, a pytorch tensor\n",
    "    if channels == 1:\n",
    "        im_input = im_i_y/255.\n",
    "        im_input = Variable(torch.from_numpy(im_input).float()).\\\n",
    "        view(1, -1, im_input.shape[0], im_input.shape[1])\n",
    "    \n",
    "    if channels ==3:\n",
    "        im_input = cv2_to_batch_tensor(img_bgr=np.array(im_in))\n",
    "        \n",
    "    print(\"im_input.shape=\",im_input.shape)    \n",
    "    im_input = im_input.cuda()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        with autocast():\n",
    "            mout = model(im_input)\n",
    "    \n",
    "    #sb(im_input.cpu(),mout[0].cpu())\n",
    "    \n",
    "    if len(mout)>1:\n",
    "        out = mout[0]\n",
    "        k=mout[1].item()*k_scale\n",
    "    else:\n",
    "        out = mout\n",
    "        k=None\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    if channels == 1:\n",
    "        out = (out*255.0).clamp(0,255)\n",
    "    \n",
    "        out = out.cpu()\n",
    "        im_y = out.data[0].numpy().astype(np.float32)\n",
    "        im_h_y = im_y[0,:,:]\n",
    "        im_h = np.array(colorize(im_h_y, im_b_ycbcr))\n",
    "\n",
    "    elif channels ==3:\n",
    "        #print(\"out=\",out.shape)\n",
    "        #print(\"out.data=\",out.data.shape)\n",
    "        #print(\"im_input.shape=\",im_input.shape)\n",
    "        #im_h = out[0].data.view(im_input.shape[2], im_input.shape[3],3).numpy().astype(np.uint8)\n",
    "        im_h = batch_tensor_to_cv2(out.cpu())\n",
    "        #print(\"im.h.shape=\",im_h.shape)\n",
    "        im_h_y, _ = decolorize(im_h)\n",
    "        \n",
    "    \n",
    "    # Calculate the PNSR for prediction\n",
    "    #psnr_predicted = PSNR(im_gt_y, im_h_y)\n",
    "    psnr_predicted = PSNR(np.array(im_gt), np.array(im_h))\n",
    "    \n",
    "    \n",
    "    # Calculate the PNSR different between bicubic interpolation and vdsr prediction\n",
    "    \n",
    "    \n",
    "    \n",
    "    blur_measure_gt = variance_of_laplacian((im_gt_y).astype(np.uint8))\n",
    "    blur_measure_bicubic = variance_of_laplacian((im_b_y).astype(np.uint8))\n",
    "    blur_measure_input = variance_of_laplacian((im_i_y).astype(np.uint8))\n",
    "    blur_measure_vdsr = variance_of_laplacian((im_h_y).astype(np.uint8))\n",
    "    \n",
    "    #percept_loss_input = percept_loss.forward(ToTensor()(im_gt),ToTensor()(im_in))\n",
    "    #percept_loss_bicubic = percept_loss.forward(ToTensor()(im_gt),ToTensor()(im_b))\n",
    "    #percept_loss_predict = percept_loss.forward(ToTensor()(im_gt),ToTensor()(im_h))\n",
    "    \n",
    "    # Colorize the grey-level image and convert into RGB mode\n",
    "\n",
    "    im_gt = Image.fromarray(im_gt_ycbcr, \"YCbCr\").convert(\"RGB\")\n",
    "    im_b = Image.fromarray(im_b_ycbcr, \"YCbCr\").convert(\"RGB\")\n",
    "    \n",
    "    if display:\n",
    "    \n",
    "        print('psnr for bicubic is {}dB'.format(psnr_bicubic))\n",
    "        print('psnr for input is {}dB'.format(psnr_input))\n",
    "        \n",
    "        print('psnr for Model is {}dB'.format(psnr_predicted))\n",
    "        print(\"PSNR improvement is {}dB\".format(psnr_predicted - psnr_input))\n",
    "\n",
    "        print(\"Sharpness Measurement GroundTruth:\",blur_measure_gt)\n",
    "        print(\"Sharpness Measurement Bicubic:\",blur_measure_bicubic)\n",
    "        print(\"Sharpness Measurement Input:\",blur_measure_input)\n",
    "        print(\"Sharpness Measurement Model:\",blur_measure_vdsr)\n",
    "        print(\"Estimate of kernel:\", k)\n",
    "        \n",
    "        #print('perception loss for bicubic is {}'.format(percept_loss_bicubic))\n",
    "        #print('perception loss for input is {}'.format(percept_loss_input))\n",
    "        \n",
    "        #print('perception loss for Model is {}'.format(percept_loss_predict))\n",
    "\n",
    "        # Result visualization\n",
    "        fig = plt.figure(figsize=(18, 16), dpi= 80)\n",
    "        ax = plt.subplot(151)\n",
    "        ax.imshow(im_gt)\n",
    "        ax.set_title(\"GT\")\n",
    "\n",
    "        ax = plt.subplot(152)\n",
    "        ax.imshow(im_b)\n",
    "        ax.set_title(\"Bicubic\")\n",
    "\n",
    "        ax = plt.subplot(153)\n",
    "        ax.imshow(im_in)\n",
    "        ax.set_title(\"Input\")\n",
    "\n",
    "        ax = plt.subplot(154)\n",
    "        ax.imshow(im_h)\n",
    "        ax.set_title(\"Output(Model)\")\n",
    "        \n",
    "        \n",
    "        ax = plt.subplot(155)\n",
    "        ax.imshow(np.clip(im_h-im_in +128,0,255))\n",
    "        ax.set_title(\"Model-Input\")\n",
    "        plt.show()\n",
    "    \n",
    "    \n",
    "    \n",
    "    return k, psnr_predicted , blur_measure_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make this load still photos too, and have them added with frame = 0\n",
    "def test_results(test_dir,scale, model, channels=1, display = False):\n",
    "    instances = []\n",
    "    psnr_bicubic_sum = 0\n",
    "    psnr_model_sum = 0\n",
    "    ct = 0\n",
    "    \n",
    "    for filename in listdir(test_dir):\n",
    "        filepath = os.path.join(test_dir,filename)\n",
    "        #print(filepath)\n",
    "                \n",
    "        if is_image_file(filepath):\n",
    "            im_gt = Image.open(filepath).convert(\"RGB\")\n",
    "            #print(im_gt.size)\n",
    "            \n",
    "            (height, width) = im_gt.size\n",
    "            im_b = im_gt.resize((int(2*height/scale), int(2*width/scale)), resample = Image.BICUBIC)\n",
    "            im_b = im_b.resize((height,width), resample = Image.BICUBIC)\n",
    "            im_b2 = pil_disk_blur(im_gt,scale)\n",
    "            \n",
    "            im_h, psnr_bicubic, psnr_predicted = display_PSNR(im_gt, im_b, im_b2, model, channels=channels, display = display)\n",
    "            psnr_bicubic_sum = psnr_bicubic_sum + psnr_bicubic\n",
    "            psnr_model_sum = psnr_model_sum + psnr_predicted\n",
    "            ct = ct +1\n",
    "            #item = {\"Filepath\":filepath, \"Type\":\"image\"}\n",
    "            #instances.append(item)\n",
    "        \n",
    "    psnr_bicubic_avg = psnr_bicubic_sum / ct\n",
    "    psnr_model_avg = psnr_model_sum / ct\n",
    "        \n",
    "    print(\"Bicubic average PSNR =\",psnr_bicubic_avg)\n",
    "    print(\"Model average PSNR =\",psnr_model_avg)\n",
    "    return \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
